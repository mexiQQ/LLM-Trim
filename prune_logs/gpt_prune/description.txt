- Training Parameters: 
  - base_model: baffo32/decapoda-research-llama-7B-hf
  - save_ckpt_log_name: gpt_prune
  - kq_mode: qr_pivot
  - reconstruct: True
  - prune_attn: False
  - prune_mlp: True
  - pruning_ratio_attn: 0.25
  - pruning_ratio_mlp: 0.25
  - block_attention_layer_start: 4
  - block_attention_layer_end: 30
  - block_mlp_layer_start: 4
  - block_mlp_layer_end: 30
  - dataset: wikitext2
  - max_seq_len: 128
  - batch_size: 64
  - nbatches: 20
  - device: cuda
  - test_before_train: False
  - eval_device: cuda
  - test_after_prune: False
  - seed: 42
  - save_model: True
  - torch_version: 2.1
